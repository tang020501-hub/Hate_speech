{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Preparation",
   "id": "1dab303db9bd8d3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T18:35:25.161947Z",
     "start_time": "2025-08-10T18:35:22.538817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"labeled_news_comments.csv\")\n",
    "\n",
    "# 去掉 URL & 多余空格\n",
    "df['body'] = df['body'].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', str(x)))\n",
    "df['body'] = df['body'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# 训练集 / 验证集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['body'], df['label'],\n",
    "    test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n"
   ],
   "id": "7f926802743c1c1a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T18:38:21.212483Z",
     "start_time": "2025-08-10T18:38:12.791398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# 1. EarlyStopping 类\n",
    "# -------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), path)  # 保存最佳模型\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# -------------------------\n",
    "# 2. 数据集类\n",
    "# -------------------------\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -------------------------\n",
    "# 3. 模型 & Tokenizer\n",
    "# -------------------------\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------\n"
   ],
   "id": "f6c7b52ef632d316",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HateDataset",
   "id": "cf645ea9ef4812b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T18:38:32.079033Z",
     "start_time": "2025-08-10T18:38:32.068230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. DataLoader\n",
    "# -------------------------\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = HateDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = HateDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------\n",
    "# 5. 优化器、调度器、早停\n",
    "# -------------------------\n",
    "EPOCHS = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "writer = SummaryWriter(log_dir=\"runs/hate_speech\")\n",
    "\n",
    "\n"
   ],
   "id": "e1a3f4a841bc573a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T19:57:05.104018Z",
     "start_time": "2025-08-10T18:38:36.646741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# 6. 训练循环\n",
    "# -------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_labels_all = [], []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        train_preds.extend(preds)\n",
    "        train_labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(train_labels_all, train_preds)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "\n",
    "    # ---- Validate ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels_all, val_preds)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss / len(val_loader), epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping ----\n",
    "    early_stopping(val_loss / len(val_loader), model, \"best_model.pth\")\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "writer.close()"
   ],
   "id": "713f12c98bdadb10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]:   0%|          | 0/3742 [00:00<?, ?it/s]D:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 1/3 [Train]: 100%|██████████| 3742/3742 [24:10<00:00,  2.58it/s]\n",
      "Epoch 1/3 [Val]:   0%|          | 0/936 [00:00<?, ?it/s]D:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 1/3 [Val]: 100%|██████████| 936/936 [01:56<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc=0.9030, Val Acc=0.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train]:   0%|          | 0/3742 [00:00<?, ?it/s]D:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 2/3 [Train]: 100%|██████████| 3742/3742 [24:12<00:00,  2.58it/s]\n",
      "Epoch 2/3 [Val]:   0%|          | 0/936 [00:00<?, ?it/s]D:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 2/3 [Val]: 100%|██████████| 936/936 [01:56<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc=0.9521, Val Acc=0.9274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train]:   0%|          | 0/3742 [00:00<?, ?it/s]D:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 3/3 [Train]: 100%|██████████| 3742/3742 [24:14<00:00,  2.57it/s]\n",
      "Epoch 3/3 [Val]:   0%|          | 0/936 [00:00<?, ?it/s]D:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 3/3 [Val]: 100%|██████████| 936/936 [01:56<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc=0.9826, Val Acc=0.9268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:11:10.492148Z",
     "start_time": "2025-08-10T13:11:09.028201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, sentiment_analyzer, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentiment_analyzer = sentiment_analyzer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        # 情感分数\n",
    "        sentiment = self.sentiment_analyzer.polarity_scores(text)\n",
    "        sentiment_vector = torch.tensor([\n",
    "            sentiment['neg'], sentiment['neu'], sentiment['pos'], sentiment['compound']\n",
    "        ], dtype=torch.float)\n",
    "\n",
    "        # BERT 编码\n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment': sentiment_vector,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ],
   "id": "46b0b1d0196edd06",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Tang\n",
      "[nltk_data]     Heqiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:11:16.951638Z",
     "start_time": "2025-08-10T13:11:16.944112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = HateDataset(train_texts, train_labels, tokenizer, sia, max_len=128)\n",
    "val_dataset = HateDataset(val_texts, val_labels, tokenizer, sia, max_len=128)\n",
    "\n",
    "# 注意：num_workers=0 防止 Windows + Jupyter 报错\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0)\n"
   ],
   "id": "e341c2d566eb0c03",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "948e2b8c616526cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:11:30.930887Z",
     "start_time": "2025-08-10T13:11:28.672941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class HateSpeechModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(HateSpeechModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # BERT [CLS] 输出 768 + 情感特征 4\n",
    "        self.fc = nn.Linear(768 + 4, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sentiment):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "        combined = torch.cat((cls_output, sentiment), dim=1)\n",
    "        x = self.dropout(combined)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HateSpeechModel(\"bert-base-uncased\", num_labels=len(set(df['label']))).to(device)\n"
   ],
   "id": "84ccf07edb7e82b9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T14:31:00.197339Z",
     "start_time": "2025-08-10T13:11:40.614531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "# 创建日志文件夹\n",
    "log_dir = f\"runs/hate_speech_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# ======== 训练循环 ========\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ======== 写入 TensorBoard ========\n",
    "    writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "    writer.add_scalar(\"LR\", optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    # ======== 学习率调度器更新 ========\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # ======== 保存最佳模型 ========\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"✅ Model saved.\")\n",
    "\n",
    "    # ======== Early stopping ========\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "writer.close()\n"
   ],
   "id": "eb6b46387db9a611",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]:   0%|          | 0/3742 [00:00<?, ?it/s]D:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 1/3 [Train]: 100%|██████████| 3742/3742 [24:29<00:00,  2.55it/s]\n",
      "Epoch 1/3 [Val]: 100%|██████████| 936/936 [01:59<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Train Acc: 0.9099 | Val Acc: 0.9228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train]: 100%|██████████| 3742/3742 [24:28<00:00,  2.55it/s]\n",
      "Epoch 2/3 [Val]: 100%|██████████| 936/936 [01:58<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Train Acc: 0.9516 | Val Acc: 0.9203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train]: 100%|██████████| 3742/3742 [24:24<00:00,  2.56it/s]\n",
      "Epoch 3/3 [Val]: 100%|██████████| 936/936 [01:58<00:00,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Train Acc: 0.9804 | Val Acc: 0.9160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T14:43:36.683784Z",
     "start_time": "2025-08-10T14:41:58.868572Z"
    }
   },
   "cell_type": "code",
   "source": "!tensorboard --logdir runs",
   "id": "6aba9a858730172e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T06:57:05.044105Z",
     "start_time": "2025-08-11T06:57:05.026554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/hate_speech\n"
   ],
   "id": "c5929b98da828ff7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 36768), started 16:13:20 ago. (Use '!kill 36768' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b449b64645f8b347\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b449b64645f8b347\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T06:57:50.489181Z",
     "start_time": "2025-08-11T06:57:50.468599Z"
    }
   },
   "cell_type": "code",
   "source": "!kill 36768",
   "id": "2f3dd8a2bfa97f8f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:16:47.430078Z",
     "start_time": "2025-08-11T12:16:46.989153Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"best_model.pt\")\n",
   "id": "3a2e605fea1c49c6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.load_state_dict(torch.load(\"best_model.pt\"))",
   "id": "4a19180241b81f57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T14:49:34.218807Z",
     "start_time": "2025-08-10T14:49:34.213228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_hate_speech(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    预测文本是否为仇恨言论\n",
    "\n",
    "    参数:\n",
    "        text (str): 要分类的文本\n",
    "        threshold (float): 判定为仇恨言论的概率阈值\n",
    "\n",
    "    返回:\n",
    "        dict: 包含预测结果和置信度\n",
    "    \"\"\"\n",
    "    # 文本编码\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    # 获取概率\n",
    "    prob_hate = probs[0][1].item()  # 假设索引1对应\"仇恨言论\"类别\n",
    "\n",
    "    # 判定结果\n",
    "    is_hate = prob_hate >= threshold\n",
    "    confidence = prob_hate if is_hate else 1 - prob_hate\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"is_hate_speech\": bool(is_hate),\n",
    "        \"confidence\": float(confidence),\n",
    "        \"hate_probability\": float(prob_hate),\n",
    "        \"threshold\": float(threshold)\n",
    "    }"
   ],
   "id": "b391d8caa7c1d73b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:55:45.799891Z",
     "start_time": "2025-08-10T17:55:45.739695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试语句\n",
    "test_texts = [\n",
    "    \"Asians are ugly!\",\n",
    "    \"You people are disgusting and should leave this country\",\n",
    "    \"This is just my opinion, but the policy seems unfair\"\n",
    "]\n",
    "\n",
    "# 批量预测\n",
    "for text in test_texts:\n",
    "    result = predict_hate_speech(text)\n",
    "    print(f\"文本: {result['text']}\")\n",
    "    print(f\"是否为仇恨言论: {result['is_hate_speech']} (置信度: {result['confidence']:.2f})\")\n",
    "    print(f\"仇恨概率: {result['hate_probability']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "5babc37e1b585d0f",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HateSpeechModel.forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# 批量预测\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m test_texts:\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     result = \u001B[43mpredict_hate_speech\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m文本: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     12\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m是否为仇恨言论: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult[\u001B[33m'\u001B[39m\u001B[33mis_hate_speech\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (置信度: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult[\u001B[33m'\u001B[39m\u001B[33mconfidence\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mpredict_hate_speech\u001B[39m\u001B[34m(text, threshold)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# 预测\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m     probs = torch.softmax(outputs.logits, dim=\u001B[32m1\u001B[39m)\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# 获取概率\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[31mTypeError\u001B[39m: HateSpeechModel.forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T18:09:23.981439Z",
     "start_time": "2025-08-10T18:09:22.389569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# 1. EarlyStopping 类\n",
    "# -------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), path)  # 保存最佳模型\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# -------------------------\n",
    "# 2. 数据集类\n",
    "# -------------------------\n",
    "class HateDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -------------------------\n",
    "# 3. 模型 & Tokenizer\n",
    "# -------------------------\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------\n",
    "# 4. DataLoader\n",
    "# -------------------------\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = HateDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = HateDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------\n",
    "# 5. 优化器、调度器、早停\n",
    "# -------------------------\n",
    "EPOCHS = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "writer = SummaryWriter(log_dir=\"runs/hate_speech\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. 训练循环\n"
   ],
   "id": "fff54fda66ab57d3",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "页面文件太小，无法完成操作。 (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 66\u001B[39m\n\u001B[32m     64\u001B[39m model_name = \u001B[33m\"\u001B[39m\u001B[33mbert-base-uncased\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     65\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m model = \u001B[43mAutoModelForSequenceClassification\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m device = torch.device(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     69\u001B[39m model.to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    598\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    599\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m600\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    601\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    602\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    604\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    605\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    606\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:315\u001B[39m, in \u001B[36mrestore_default_torch_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    313\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    314\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m315\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    317\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Tang Heqiang\\Documents\\Assignment\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4884\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4876\u001B[39m is_from_file = pretrained_model_name_or_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4878\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   4879\u001B[39m     is_safetensors_available()\n\u001B[32m   4880\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m is_from_file\n\u001B[32m   4881\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sharded\n\u001B[32m   4882\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m checkpoint_files[\u001B[32m0\u001B[39m].endswith(\u001B[33m\"\u001B[39m\u001B[33m.safetensors\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   4883\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m4884\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43msafe_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframework\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m   4885\u001B[39m         metadata = f.metadata()\n\u001B[32m   4887\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m metadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   4888\u001B[39m         \u001B[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001B[39;00m\n",
      "\u001B[31mOSError\u001B[39m: 页面文件太小，无法完成操作。 (os error 1455)"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_labels_all = [], []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        train_preds.extend(preds)\n",
    "        train_labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(train_labels_all, train_preds)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "\n",
    "    # ---- Validate ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels_all, val_preds)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss / len(val_loader), epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping ----\n",
    "    early_stopping(val_loss / len(val_loader), model, \"best_model.pth\")\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "writer.close()\n"
   ],
   "id": "aa9fe94de33aeb7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ba06081eea51dfaa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
